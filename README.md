# Assignment-3

## Live application Links
[![codelabs](https://img.shields.io/badge/codelabs-4285F4?style=for-the-badge&logo=codelabs&logoColor=white)](https://codelabs-preview.appspot.com/?file_id=1vkNzPBXibaYNVNK3z4BwVSpV7XNYXeLTjbUcQXptyOI#0)


## Problem Statement
Design Python classes for web page and Grobid output schemas, validating data with Pydantic and creating "clean" CSV files. Utilize DBT with Snowflake to load clean data, construct a summary table, and deploy the model with testing. Establish Test and Production Environments in Snowflake, considering necessary considerations for deployment.

## Project Goals
Design and implement Python classes, namely URLClass, MetaDataPDFClass, and ContentPDFClass, to represent the schema for webpages and Grobid output (Part 1). Utilize Pydantic for data and schema validation, generating "clean" CSV files, and creating 5+5 test cases for validation success/failure. For Part 2, use DBT with Snowflake to load clean data, create a summary table schema, materialize it, write tests, and document the model. Additionally, set up Test and Production Environments in Snowflake, considering relevant considerations for each.

## Technologies Used
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/)
[![Python](https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=blue)](https://www.python.org/)
[![DBT](https://img.shields.io/badge/DBT-861121?style=for-the-badge)](https://www.getdbt.com/)
[![GROBID](https://img.shields.io/badge/GROBID-FFFFFF?style=for-the-badge&logo=GROBID&logoColor=black)](https://grobid.readthedocs.io/en/latest/Introduction/)
[![Snowflake](https://img.shields.io/badge/snowflake-0000FF?style=for-the-badge&logo=snowflake&logoColor=white)](https://docs.snowflake.com/ )

## Data Sources
[CFA Institute's website](https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#sort=%40refreadingcurriculumyear%20descending)

## Pre requisites
1. Python Knowledge
2. Snowflake Account
3. DBT Cloud Knowledge

## Project Structure
```
ðŸ“¦ Assignment3
â”œâ”€Â analyses
â”‚Â Â â””â”€Â .gitkeep
â”œâ”€Â data
â”‚Â Â â”œâ”€Â clean-data
â”‚Â Â â”‚Â Â â”œâ”€Â cfa-data-clean.csv
â”‚Â Â â”‚Â Â â”œâ”€Â grobid_metadata-clean.csv
â”‚Â Â â”‚Â Â â”œâ”€Â pdf-data-grobid-clean.csv
â”‚Â Â â”‚Â Â â””â”€Â pdf-data-pypdf-clean.csv
â”‚Â Â â”œâ”€Â pdf_raw_input
â”‚Â Â â”‚Â Â â”œâ”€Â pypdf_outputs
â”‚Â Â â”‚Â Â â””â”€Â xml_files
â”‚Â Â â”œâ”€Â pdf_raw_output
â”‚Â Â â”‚Â Â â”œâ”€Â grobid_metadata.csv
â”‚Â Â â”‚Â Â â”œâ”€Â grobid_output.csv
â”‚Â Â â”‚Â Â â””â”€Â pypdf_output.csv
â”‚Â Â â””â”€Â raw_data
â”œâ”€Â diagram
â”‚Â Â â”œâ”€Â architecture_diagram.ipynb
â”‚Â Â â””â”€Â architecture_diagram.png
â”œâ”€Â logs
â”‚Â Â â””â”€Â data-validation-cleaning.log
â”œâ”€Â macros
â”œâ”€Â models
â”‚Â Â â”œâ”€Â scrapped_grobbid.sql
â”‚Â Â â”œâ”€Â scrapped_grobbid_schema.yml
â”‚Â Â â”œâ”€Â scrapped_pypdf.sql
â”‚Â Â â””â”€Â scrapped_pypdf_schema.yml
â”œâ”€Â models.py
â”‚Â Â â”œâ”€Â cfa_data_model.py
â”‚Â Â â”œâ”€Â metadata_data_model.py
â”‚Â Â â””â”€Â pdf_data_model.py
â”œâ”€Â notebooks
â”‚Â Â â”œâ”€Â content_extracter.ipynb
â”‚Â Â â”œâ”€Â snowflake_helper.py
â”‚Â Â â”œâ”€Â snowflake_upload.ipynb
â”‚Â Â â””â”€Â upload_to_S3.ipynb
â”œâ”€Â script
â”‚Â Â â”œâ”€Â custom_logger.py
â”‚Â Â â””â”€Â data_cleaner.py
â””â”€Â .github/workflows
```
Â©generated by [Project Tree Generator](https://woochanleee.github.io/project-tree-generator)

## How to run Application locally 
1. Create Python virtual environment
2. run -> pip install -r requirements.txt
3. load Jupyter Notebook
5. run local image of Grobid
6. run the Jupyter notebooks present under the notebooks directory
7. In terminal - export PYTHONPATH=.
8. run data_cleaner.py under scripts
9. Run snowflake upload to upload data
10. Run DBT sql models to run
11. Deploy, test - for test env, prod for production environment
12. In the snowflake outputs will be visible under ASSIGNMENT3, test and prod schema

## Project run outline

### 1. CFA Data - Cleaning, Validation, Testing
- Created pydantic class in order to validate data
- Pytest to test the class with covering edge cases
- Claeaning test to run raw data to validate using Pydantic

### 2. PDF Data - Extraction to Structured Schema, Metadata Extraction, Cleaning, Validation, Testing
- Extract the pdf contents from Grobid output and store it in a structured schema
- Extract the metadata from Grobid using Langchain and strore it in a structured schema
- Perform Validation using Pydantic and Testing
- Additionally, Extraction of pdf contens from Pypdf output to a structured schema has been done
  
### 3. SnowFlake
- From the CSV data files created snowflake databases SQLAlchemy
- Creted separate DBs for test and prod deployment output

### 4. DBT
- Connection of snowflake with dbt
- Creation of models for asked schema
- Generated test
- Deployed to test and prod

CodeLab - [Documentation](https://docs.google.com/document/d/1a4kE9iRo0cuh8gUI4NTd2sVjGIwXH5tBalrgW-4uvd0/edit#heading=h.j0flkct7g8l6) 

## References

- https://www.getdbt.com/
- https://docs.getdbt.com/guides/snowflake?step=1
- https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- https://www.selenium.dev/
- https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#sort=%40refreadingcurriculumyear%20descending
- https://github.com/kermitt2/grobid
- https://diagrams.mingrammer.com/
- https://app.snowflake.com/
- https://github.com/ashrithagoramane/DAMG7245-Spring24/tree/main/repository_structure

  ## Team Information 
  Project Board - https://github.com/orgs/BigDataIA-Spring2024-Sec2-Team2/projects/2/views/1
  
  Name | Contribution %| Contributions |
  --- |--- | --- |
  Anshul Chaudhary  | 35 % | Classes, Pydantic Models, Pytest, Cleaning , Snowflake and S3|
  Agash Uthayasuriyan | 35% | PDF content Extraction (Grobid, PyPDF), Metadata Extraction, Pydantic Validation|
  Narayani Arun Patil | 30% | Part 2, Deployments to test and prod and snowflake DBs |
